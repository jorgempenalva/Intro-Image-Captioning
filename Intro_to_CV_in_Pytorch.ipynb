{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![alt text](images/slides/intro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Table of Contents\n",
    "- Intro to PyTorch\n",
    "    - Hello World\n",
    "    - Auto Grad\n",
    "    - Linear Regression\n",
    "    - Gradient Descent\n",
    "    - Logistic Regression\n",
    "    - Data loaders for SGD\n",
    "    - Two layer neural network\n",
    "- CNNs\n",
    "- Image Classification\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intro to PyTorch\n",
    "\n",
    "Apart from the general Python imports for math, random, Pandas, and and Numpy, we will import the Torch utilities. These include the torch, torch.nn, and torch.utils for the Dataset loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import pytorch libraries\n",
    "%matplotlib inline\n",
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import random;\n",
    "import math;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PyTorch consists of 4 main packages:**\n",
    "\n",
    "- torch: a general purpose array library similar to Numpy that can do computations on GPU\n",
    "- torch.autograd: a package for automatically obtaining gradients\n",
    "- torch.nn: a neural net library with common layers and cost functions\n",
    "- torch.optim: an optimization package with common optimization algorithms like SGD, Adam, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hello World in PyTorch\n",
    "\n",
    "Instead of a traditional \"Hello World\" print statement, we can check PyTorch is functioning properly by creating and outputing a Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create random tensor\n",
    "N = 5\n",
    "x = torch.randn(N, 10).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "PyTorch tensors are like Numpy tensors but they can utilize GPUs to accelerate numerical computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6736,  0.6488, -1.4323, -0.4892, -1.8398, -0.5002,  0.6992,  2.0750,\n",
       "          1.0564,  0.4420, -1.9077, -0.9362, -0.9655,  1.2114,  2.4377,  1.8508,\n",
       "          0.4975,  0.1614,  0.5360, -0.3479, -0.5898,  0.3874, -0.8302, -0.7594,\n",
       "         -0.8495,  0.1700,  1.5493, -0.1895,  0.2229,  0.8790,  0.2944, -1.3832,\n",
       "          1.0332, -0.0316, -1.5368, -0.5422,  1.2362,  0.0118,  0.7233, -0.3437,\n",
       "         -1.4607, -1.0255, -1.2584,  0.1984,  0.1661,  0.1159, -0.5904, -1.5080,\n",
       "          0.7930, -0.3617]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshaping of tensors using .view()\n",
    "x.view(1,-1) #-1 makes torch infer the second dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pytorch Autograd\n",
    "\n",
    "The autograd package in PyTorch automates the computation of backward passes in neural networks. That is, the gradient of the error with respect to all parameters.\n",
    "\n",
    "In order for this to happen we need to wrapp up our data and paramerers with the aurograd.Variable() function. Each variable has a .grad property which is itâ€™s gradient with respect to our Error function. Each variable also has a .data property which is it the actual value of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3., 4., 5., 6.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(48., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = (2*x+1).sum()\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression with Pytorch\n",
    "\n",
    "The goal of linear regression is to fit a line to a set of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Here we generate some fake data\n",
    "def lin(a,b,x): return a*x+b\n",
    "\n",
    "def gen_fake_data(n, a, b):\n",
    "    x = np.random.uniform(0,1,n) \n",
    "    y = lin(a,b,x) + 0.1 * np.random.normal(0,3,n)\n",
    "    return x, y\n",
    "\n",
    "x, y = gen_fake_data(50, 3., 8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1169dfe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,y, s=8); plt.xlabel(\"x\"); plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You want to find parameters (weights) $a$ and $b$ such that you minimize the error between the points and the line $a\\cdot x + b$. Note that here $a$ and $b$ are unknown. For a regression problem the most common error function or loss function is the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def mse(y_hat, y): return ((y_hat - y) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we believe $a = 10$ and $b = 5$ then we can compute y_hat which is our prediction and then compute our error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.59674385024895"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = lin(10,5,x)\n",
    "mse(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def mse_loss(a, b, x, y): return mse(lin(a,b,x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.59674385024895"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss(10, 5, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So far we have specified the model (linear regression) and the evaluation criteria (or loss function). Now we need to handle optimization; that is, how do we find the best values for $a$ and $b$? How do we find the best fitting linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## References\n",
    "\n",
    "- http://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "- https://hsaghir.github.io/data_science/pytorch_starter/\n",
    "- http://a.sjtume.cn/2017/PyTorch-CNN/\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
