{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop NLP lang.ai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spacy for preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![architecture_spacy](img/spacy_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing Caesar cypher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spacy tokenization, lemmatization, dependency tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this DET this nsubj\n",
      "is VERB be ROOT\n",
      "one NUM one attr\n",
      "of ADP of prep\n",
      "the DET the det\n",
      "examples NOUN example pobj\n",
      "of ADP of prep\n",
      "tokenization NOUN tokenization pobj\n",
      ". PUNCT . punct\n",
      "U.K PROPN u.k nsubj\n",
      "won VERB win ROOT\n",
      "in ADP in prep\n",
      "the DET the det\n",
      "World PROPN world compound\n",
      "Cup PROPN cup pobj\n",
      "yesterday NOUN yesterday npadvmod\n",
      ". PUNCT . punct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc=nlp(\"this is one of the examples of tokenization. U.K won in the World Cup yesterday.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.lemma_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nuances of tokenization/lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -PRON-\n",
      "want want\n",
      "to to\n",
      "fly fly\n",
      "from from\n",
      "Spain spain\n",
      "to to\n",
      "Mexico mexico\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"I want to fly from Spain to Mexico\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -PRON-\n",
      "flew fly\n",
      "from from\n",
      "Spain spain\n",
      "to to\n",
      "Mexico mexico\n",
      "and and\n",
      "I -PRON-\n",
      "lost lose\n",
      "my -PRON-\n",
      "luggage luggage\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"I flew from Spain to Mexico and I lost my luggage\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevance of dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -PRON- nsubj\n",
      "flew fly ROOT\n",
      "from from prep\n",
      "Spain spain pobj\n",
      "to to prep\n",
      "Mexico mexico pobj\n",
      "and and cc\n",
      "I -PRON- nsubj\n",
      "lost lose conj\n",
      "my -PRON- poss\n",
      "luggage luggage dobj\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"I flew from Spain to Mexico and I lost my luggage\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -PRON- nsubj\n",
      "want want ROOT\n",
      "to to aux\n",
      "book book xcomp\n",
      "a a det\n",
      "flight flight dobj\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"I want to book a flight\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -PRON- nsubj\n",
      "want want ROOT\n",
      "to to aux\n",
      "order order xcomp\n",
      "a a det\n",
      "ham ham nmod\n",
      "and and cc\n",
      "cheese cheese conj\n",
      "pizza pizza dobj\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"I want to order a ham and cheese pizza\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -PRON- nsubj\n",
      "hope hope ROOT\n",
      "the the det\n",
      "fact fact dobj\n",
      "that that mark\n",
      "I -PRON- nsubj\n",
      "want want acl\n",
      "a a det\n",
      "bbq bbq compound\n",
      "ckn ckn compound\n",
      "pizza pizza dobj\n",
      "from from prep\n",
      "Blaze blaze pobj\n",
      ", , punct\n",
      "some some det\n",
      "chicken chicken compound\n",
      "wings wing appos\n",
      "from from prep\n",
      "Ale ale compound\n",
      "Mary mary compound\n",
      "’s ’s nsubj\n",
      "and and cc\n",
      "a a det\n",
      "taco taco conj\n",
      "means mean ROOT\n",
      "I -PRON- nsubj\n",
      "miss miss ccomp\n",
      "Royal royal compound\n",
      "Oak oak dobj\n",
      "and and cc\n",
      "not not neg\n",
      "that that mark\n",
      "I -PRON- nsubj\n",
      "’m be aux\n",
      "expecting expect conj\n",
      ". . punct\n",
      "I -PRON- nsubj\n",
      "’m be ROOT\n",
      "hungry hungry acomp\n",
      ". . punct\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"I hope the fact that I want a bbq ckn pizza from Blaze, some chicken wings from Ale Mary’s and a taco means I miss Royal Oak and not that I’m expecting. I’m hungry.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.dep_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
